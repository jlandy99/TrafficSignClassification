# -*- coding: utf-8 -*-
"""TrafficSignRecognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xgn899Z6nCofB2-28moYFMY8Gh1TsWOE

# Traffic Sign Recognition

Handle necessary imports
"""

import os
# import time

import pandas as pd

import torch.nn as nn
from torchsummary import summary
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms
from torch.utils.data.dataset import Dataset
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score
import torch.optim as optim

from PIL import Image

from helper import *

# from google.colab import drive

"""Make sure we are leveraging the hosted GPU"""

# if torch.cuda.is_available():
#     print("Using the GPU. You are good to go!")
#     device = torch.device('cpu')
# else:
# #     raise Exception("WARNING: Could not find GPU! Using CPU only. \
# # To enable GPU, please to go Edit > Notebook Settings > Hardware \
# # Accelerator and select GPU.")
#     device = torch.device('cuda:0')


"""We are using the The German Traffic Sign Recognition Benchmark as our dataset which contains over 50,000 images of localized traffic signs across 40 different classes:
https://benchmark.ini.rub.de/gtsrb_dataset.html. This raw dataset is uploaded into a shared drive and here we will preprocess it into numpy arrays. If the data has already been processed, then this code block will identify that and not redo it. We will save the the training data into a single numpy file Data/X_train and Data/Y_train
"""

# Mount the shared drive folder to access the data
# drive.mount('/content/shared_drives/', force_remount=True)
# root_path = 'shared_drives/Shareddrives/TrafficSignRecognition/'
# data_dir = os.path.join(root_path, 'Data')

processed_path = 'Data/ProcessedData'
X_processed_path = os.path.join(processed_path, 'X.npy')
Y_processed_path = os.path.join(processed_path, 'Y.npy')

X_processed_path_exists = os.path.exists(X_processed_path)
Y_processed_path_exists = os.path.exists(Y_processed_path)

# this will skip processing if our training data already has been processed
# because this will be a time consuming process and obviously do not want to
# do this if we do not have to and has already been done
X_processed = None
Y_processed = None

# we need to have a constant image size that we are up/downsampling to
# TODO DETERMINE WHAT IS A GOOD SIZE (probably what seems the most common)
IMAGE_DIM = 64
NUM_TRAIN_IMAGES = 39209
NUM_TEST_IMAGES = 12630
NUM_TOTAL_IMAGES = NUM_TRAIN_IMAGES + NUM_TEST_IMAGES

if X_processed_path_exists and Y_processed_path_exists:

  print("X_processed and Y_processed already exist, loading from files")
  X_processed = np.load(X_processed_path)
  Y_processed = np.load(Y_processed_path)

  assert X_processed.shape == (NUM_TOTAL_IMAGES, 3, IMAGE_DIM, IMAGE_DIM)
  assert Y_processed.shape == (NUM_TOTAL_IMAGES,)

  print("X_processed, Y_processed loaded successfully")

else:

  print("X_processed or Y_processed does not exist, processing raw data")
  print("processing training portion")

  X_processed = np.empty((NUM_TOTAL_IMAGES, 3, IMAGE_DIM, IMAGE_DIM))
  Y_processed = np.empty(NUM_TOTAL_IMAGES)

  # Iterate through each sub folder and access each sub folder's CSV file
  # which contains the image filenames in that subfolder as well as each
  # image's correct classification
  raw_train_dir = 'Data/RawTraining'

  index = 0

  for subfolder in os.listdir(raw_train_dir):

    print('processing', subfolder)

    csv_filename = os.path.join(os.path.join(raw_train_dir, subfolder), 'GT-' + subfolder + '.csv')
    df = pd.read_csv(csv_filename, delimiter=';')

    for _ , row in df.iterrows():

      # open image up/downsample to standard size and insert into X_train
      # record label in Y_train
      img_path = os.path.join(os.path.join(raw_train_dir, subfolder), row['Filename'])
      img = Image.open(img_path).resize((IMAGE_DIM, IMAGE_DIM))

      # RGB channel needs to be second to be compatible with torch net
      X_processed[index] = np.moveaxis(np.asarray(img), -1, 0)
      Y_processed[index] = row['ClassId']
      
      index += 1


  assert index == NUM_TRAIN_IMAGES

  # np.save(X_train_path, X_train)
  # np.save(Y_train_path, Y_train)

  print("X training and Y training processed successfully")
  print("processing testing portion")

  # For testing all images are in one subfolder unlike training
  # in which they were a bunch of different subfolders
  raw_test_dir = 'Data/RawTesting'

  csv_filename = os.path.join(raw_test_dir, 'GT-final_test.csv')
  df = pd.read_csv(csv_filename, delimiter=';')


  for _ , row in df.iterrows():

    # open image up/downsample to standard size and insert into X_test, Y_test
    img_path = os.path.join(raw_test_dir, row['Filename'])
    img = Image.open(img_path).resize((IMAGE_DIM, IMAGE_DIM))

    # RGB channel needs to be second to be compatible with torch net
    X_processed[index] = np.moveaxis(np.asarray(img), -1, 0)
    Y_processed[index] = row['ClassId']
    
    index += 1

    # print some updates to console
    if index % 500 == 0:
      print("processed", index, "testing images")


  assert index == NUM_TOTAL_IMAGES

  np.save(X_processed_path, X_processed)
  np.save(Y_processed_path, Y_processed)

  print("Successfully processed all raw data and saved to numpy arrays for future loading")


"""Now we will analyze the distribution of the classes in the training and testing data. We want to make sure the dataset is balanced and not heavily biased towards a specific class so we will balance the dataset here"""

N_CLASS = 43

print('\nCombined Dataset Statistics -----------------------------------------')
plot_class_dist_and_stats(Y_processed, N_CLASS, 'original_distribution.jpg')


"""Here we can see out dataset is incredibly unbalanced. Completely balancing the dataset would cause us to lose a lot of data so we wll cap the number of training samples for a class at the median."""

class_max_samples = int(np.median(np.bincount(Y_processed.astype(np.uint8))))

Y_processed_copy = Y_processed.copy().astype(np.uint8)

indices_to_keep = []
for i in range(N_CLASS):

  samples = np.argwhere(Y_processed_copy == i).flatten()
  if samples.shape[0] > class_max_samples:
    samples = samples[:class_max_samples]
  
  indices_to_keep.extend(list(samples))


# sort the new indices
indices_to_keep.sort()

# These variables will be used for training
new_X_processed = X_processed[indices_to_keep]
new_Y_processed = Y_processed[indices_to_keep]
NEW_NUM_TOTAL_IMAGES = new_Y_processed.shape[0]

print("\nMore Balanced Combined Dataset Statistics")
plot_class_dist_and_stats(new_Y_processed, N_CLASS, 'balanced_distribution.jpg')

"""DEFINE MODEL TODO MAKE IT LOOK LIKE HOW THIS IS SET UP IN PART 3 OF LAST HOMEWORK"""

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.n_class = N_CLASS
    self.layers = nn.Sequential(
        # TODO THIS IS SUPER SIMPLE JUST FOR TESTING
        nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1),
        nn.MaxPool2d(kernel_size=2, stride=2),
        nn.Conv2d(in_channels=8, out_channels=64, kernel_size=3, padding=1),
        nn.MaxPool2d(kernel_size=2, stride=2),

        nn.Flatten(),

        nn.Linear(in_features=16384, out_features=1024),
        nn.Dropout(p=0.4),
        nn.ReLU(inplace=True),
        nn.Linear(in_features=1024, out_features=N_CLASS)
        )
    

  def forward(self, x):
    return self.layers(x)


"""Initialize model and look at structure and summary TODO THIS IS ASSUMING WE ARE USING TORCH"""

# net = Net().to(device)
net = Net()
# visualizing the model
print('Your network:')
summary(net, (3,64,64))

"""Create a custom Dataset so we can use Dataloader, somewhat similarly to done in HW5"""

# TODO IM PRETY SURE THIS IS RIGHT BUT IDK
class TrafficSignDataset(Dataset):

  def __init__(self, X, y, data_range):

      # skLearn.utils.shuffle "Shuffles arrays or sparse matrices in a consistent way"
      # we want to shuffle the arrays so they are not always the exact same when training
      # make sures numpy arrays are uint8 type so arent excessvely big for pixel data
      # TODO WE MIGHT NOT NEED THIS CUZ DATALOADER MIGHT SHUFFLE ANYWAYS
      X, y = shuffle(X.astype(np.uint8), y.astype(np.uint8))

      self.X = torch.from_numpy(X[data_range[0]:data_range[1]]).float()
      self.y = torch.from_numpy(y[data_range[0]:data_range[1]]).long()

  def __len__(self):
      return len(self.X)

  def __getitem__(self, index):
      return self.X[index], self.y[index]


##############################################################################
# Now use DataLoader on our custom dataset class
# TODO WE NEED TO GET TEST DATA

TRAIN_RATIO = 0.8
VAL_RATIO = 0.1
TEST_RATIO = 0.1

TRAIN_SPLIT_SIZE = int(NUM_TOTAL_IMAGES * TRAIN_RATIO)
VAL_SPLIT_SIZE = int(NUM_TOTAL_IMAGES * VAL_RATIO)

train_range = (0, TRAIN_SPLIT_SIZE)
val_range = (TRAIN_SPLIT_SIZE, TRAIN_SPLIT_SIZE + VAL_SPLIT_SIZE)
test_range = (TRAIN_SPLIT_SIZE + VAL_SPLIT_SIZE, NUM_TOTAL_IMAGES)

train_data = TrafficSignDataset(X_processed, Y_processed, train_range)
val_data = TrafficSignDataset(X_processed, Y_processed, val_range)
test_data = TrafficSignDataset(X_processed, Y_processed, test_range)

train_loader = DataLoader(train_data, batch_size=int(TRAIN_SPLIT_SIZE / 40))
val_loader = DataLoader(val_data, batch_size=int(VAL_SPLIT_SIZE / 40))
test_loader = DataLoader(test_data, batch_size=1)


"""EPOCH LOOP FROM HW5 part 3"""

# Tunable parameters #####################################################
criterion = nn.CrossEntropyLoss()
learning_rate = 1e-5
weight_decay = 1e-5
num_epoch = 10
# ########################################################################

optimizer = torch.optim.Adam(net.parameters(), learning_rate, weight_decay=weight_decay)


print('\nStart training')

train_loss_history = []
val_loss_history = []

train_acc_history = []
val_acc_history = []

for epoch in range(num_epoch):
  print('-----------------Epoch = %d-----------------' % (epoch+1))

  # train_loss = train(train_loader, net, criterion, optimizer, device, epoch+1)
  train_loss = train(train_loader, net, criterion, optimizer, epoch+1)

  print('Train accuracy: ')

  # train_acc = cal_accuracy(train_loader, net, criterion, device)
  train_acc = cal_accuracy(train_loader, net, criterion)

  print(train_acc)

  print('Validation loss: ')

  # val_loss = test(val_loader, net, criterion, device)
  val_loss = test(val_loader, net, criterion)


  print('Validation accuracy: ')

  # val_acc = cal_accuracy(train_loader, net, criterion, device)
  val_acc = cal_accuracy(train_loader, net, criterion)

  print(val_acc)


  train_loss_history.append(train_loss)
  train_acc_history.append(train_acc)

  val_loss_history.append(val_loss)
  val_acc_history.append(val_acc)

  


plot_history(train_loss_history, val_loss_history, 'train_loss.jpg', loss=True)
plot_history(train_acc_history, val_acc_history, 'train_acc.jpg')


print('\nFinished Training, Testing on test set')

# print('\nFinal Test Set Accuracy:', cal_accuracy(test_loader, net, criterion, device))
print('\nFinal Test Set Accuracy:', cal_accuracy(test_loader, net, criterion))

